{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a223c0-dcab-4f65-8642-30e61c262756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer ,TFAutoModelForMultipleChoice\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1280a9-fa38-45ca-b2ba-bb70d74b1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a27d0220-5a3d-4c2b-a1f6-324806ee1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791151d6-2f2f-44da-943b-2c7e09642eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca21d42-c19b-4c61-bd58-81e84837166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e89d3ed-7e24-4f86-9cb8-65b4f96868c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMultipleChoice.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForMultipleChoice were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForMultipleChoice.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac7db5e-682d-4952-8a66-548889f7d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbebb49d-a665-41b3-8d6f-20170bf1bf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8387b80b-d7b6-4b4c-a528-33a90e4cea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c087546-ee8c-4ae5-94f5-7f03d0bf0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_options = df[['A','B','C','D','E']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fc838c-97a4-40b3-bebb-80bbd22b57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = encoder.fit_transform(df.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db693717-f12f-40d3-a0ae-b360a7952fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q,val_q,train_ans_opt, val_ans_opt, train_labels,val_labels = train_test_split(\n",
    "    questions,\n",
    "    answer_options,\n",
    "    answer,\n",
    "    test_size=0.2,\n",
    "    random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feeca957-6e6f-4a53-9b70-f20fca7f21b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Heisenberg uncertainty principle states that the axis of rotation of a quantum particle is undefined, and that quantum particles possess a type of non-orbital angular momentum called \"spin\". This is because angular momentum, like other quantities in quantum mechanics, is expressed as a tensorial operator in relativistic quantum mechanics.',\n",
       " 'The Heisenberg uncertainty principle states that the total angular momentum of a system of particles is equal to the sum of the individual particle angular momenta, and that the centre of mass is for the system. This is because angular momentum, like other quantities in quantum mechanics, is expressed as an operator with quantized eigenvalues.',\n",
       " 'The Heisenberg uncertainty principle states that the total angular momentum of a system of particles is subject to quantization, and that the individual particle angular momenta are expressed as operators. This is because angular momentum, like other quantities in quantum mechanics, is subject to the Heisenberg uncertainty principle.',\n",
       " 'The Heisenberg uncertainty principle states that the axis of rotation of a quantum particle is undefined, and that at any given time, only one projection of angular momentum can be measured with definite precision, while the other two remain uncertain. This is because angular momentum, like other quantities in quantum mechanics, is subject to quantization and expressed as an operator with quantized eigenvalues.',\n",
       " 'The Heisenberg uncertainty principle states that at any given time, only one projection of angular momentum can be measured with definite precision, while the other two remain uncertain. This is because angular momentum, like other quantities in quantum mechanics, is expressed as an operator with quantized eigenvalues.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_opt = train_ans_opt[0]\n",
    "train_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20e778ad-e0b5-463c-9a9d-694ae81f3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5c0d73a-771d-48db-89f0-0ffb84993f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████| 160/160 [10:18<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Training loss: 1.7975\n",
      "  Validation loss: 1.7928\n",
      "  Validation accuracy: 0.1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████████| 160/160 [10:30<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "  Training loss: 1.6841\n",
      "  Validation loss: 1.8093\n",
      "  Validation accuracy: 0.1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████| 160/160 [10:28<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Training loss: 1.2459\n",
      "  Validation loss: 2.3157\n",
      "  Validation accuracy: 0.1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████████████| 160/160 [10:28<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "  Training loss: 0.7535\n",
      "  Validation loss: 2.0368\n",
      "  Validation accuracy: 0.1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████████████| 160/160 [10:39<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "  Training loss: 0.4072\n",
      "  Validation loss: 1.9945\n",
      "  Validation accuracy: 0.1750\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for i in tqdm(range(len(train_q)), desc = f'Epoch {epoch+1}'):\n",
    "        question = train_q[i]\n",
    "        answer_options_batch = train_ans_opt[i]\n",
    "        label = train_labels[i]\n",
    "\n",
    "        questions_encoded = tokenizer(question,padding='max_length',truncation=True,max_length=150,return_tensors='tf')\n",
    "        answer_option_encoded = tokenizer(answer_options_batch, padding='max_length', truncation=True, max_length=150, return_tensors='tf')\n",
    "        \n",
    "        answer_input_ids = []\n",
    "        answer_attention_masks = []\n",
    "        for answer_option in answer_options_batch:\n",
    "            answer_encoded = tokenizer(answer_option, padding='max_length', truncation=True, max_length=150, return_tensors='tf')\n",
    "            answer_input_ids.append(answer_encoded['input_ids'])\n",
    "            answer_attention_masks.append(answer_encoded['attention_mask'])\n",
    "\n",
    "        answer_input_ids = tf.stack(answer_input_ids)\n",
    "        answer_attention_masks = tf.stack(answer_attention_masks)\n",
    "\n",
    "        input_ids = tf.concat([tf.expand_dims(questions_encoded['input_ids'], axis=0), answer_input_ids], axis=0)\n",
    "        attention_mask = tf.concat([tf.expand_dims(questions_encoded['attention_mask'], axis=0), answer_attention_masks], axis=0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,labels = tf.convert_to_tensor([label]))\n",
    "            logits = outputs.logits\n",
    "            loss = loss_func(tf.convert_to_tensor([label]), logits)\n",
    "\n",
    "        gradients = tape.gradient(loss,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        train_loss += loss.numpy()\n",
    "\n",
    "\n",
    "    for i in range(len(val_q)):\n",
    "        question = val_q[i]\n",
    "        answer_option_batch = val_ans_opt[i]\n",
    "        label = val_labels[i]\n",
    "\n",
    "        questions_encoded = tokenizer(question,padding='max_length',truncation=True,max_length=150,return_tensors='tf')\n",
    "        answer_option_encoded = tokenizer(answer_option_batch,padding='max_length',truncation=True,max_length=150,return_tensors='tf')\n",
    "        \n",
    "        answer_input_ids = []\n",
    "        answer_attention_mask = []\n",
    "        for answer_option in answer_option_batch:\n",
    "            answer_encoded = tokenizer(answer_option,padding='max_length',truncation=True ,max_length = 150, return_tensors='tf')\n",
    "            answer_input_ids.append(answer_encoded['input_ids'])\n",
    "            answer_attention_mask.append(answer_encoded['attention_mask'])\n",
    "            \n",
    "        answer_input_ids = tf.stack(answer_input_ids)\n",
    "        answer_attention_mask = tf.stack(answer_attention_mask)\n",
    "\n",
    "        input_ids = tf.concat([tf.expand_dims(questions_encoded['input_ids'], axis=0), answer_input_ids], axis=0)\n",
    "        attention_mask = tf.concat([tf.expand_dims(questions_encoded['attention_mask'], axis=0), answer_attention_masks], axis=0)\n",
    "        \n",
    "        \n",
    "        outputs = model(input_ids=input_ids,attention_mask=attention_mask,labels = tf.convert_to_tensor([label]))\n",
    "        logits = outputs.logits\n",
    "        loss = loss_func(tf.convert_to_tensor([label]), logits)\n",
    "        val_loss += loss.numpy()\n",
    "\n",
    "        predicted_label = tf.argmax(logits,axis = -1)[0]\n",
    "        if predicted_label == label:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_q)\n",
    "    avg_val_loss = val_loss / len(val_q)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Training loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b20d3-db91-4c29-8d77-3a912ef845b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
